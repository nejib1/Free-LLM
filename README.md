# üöÄ Free LLM API Directory

> **The comprehensive, automated list of free LLM APIs and models.**  
> *Last updated: 2026-02-19*

[![Website](https://img.shields.io/badge/Website-free--llm.com-blue?style=for-the-badge)](https://free-llm.com)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Auto-Updated](https://img.shields.io/badge/Updates-Daily-orange?style=for-the-badge)](#)

## üìñ Overview

This repository contains a curated list of providers offering **free access** to Large Language Models (LLMs) via API.  
The list is **automatically synchronized** with the latest resources from the community and verified availability.

For a better viewing experience with filters and comparisons, visit our website:  
üëâ **[https://free-llm.com](https://free-llm.com)**

---

## üìö Table of Contents

- [Verified Free APIs](#verified-free-apis)
- [Full Provider List](#full-provider-list)
- [Contributing](#contributing)

---
## ‚ö° Verified Free APIs

These providers offer **truly free** access (no credit card, no trial expiration) to powerful models.

| Provider | Description | Limits | Verified |
| :--- | :--- | :--- | :---: |
| **[Cerebras](https://cerebras.ai/inference)** | Cerebras Systems offers the world's fastest AI inference service, powered by ... | 30 RPM | ‚úÖ |
| **[Google AI Studio](https://aistudio.google.com/)** | Google AI Studio is a web-based prototyping environment for developers to exp... | 2-15 RPM | ‚úÖ |
| **[Groq](https://console.groq.com/)** | LPU Inference Engine. | 30 RPM, 14.4k RPD | ‚úÖ |
| **[Hugging Face Inference](https://huggingface.co/inference-api/serverless)** | The Hugging Face Serverless Inference API allows you to access over 100,000 p... | 300 Requests / hour | ‚úÖ |
| **[OpenRouter](https://openrouter.ai/)** | A community-focused router for LLMs. Offers a specific list of truly free mod... | 20 requests/minute | ‚úÖ |
| **[Together.AI](https://together.ai/)** | Access specific free research models from ServiceNow and others. Currently ho... | Subject to availability | ‚úÖ |

---

## üìã Full Provider List

### [AI21 Labs](https://docs.ai21.com/)

**Description:** Creators of the Jamba model family, the world's first production-grade Mamba-based LLMs. Offers massive context windows with high throughput. $10 free credits for new users.  
**Limits:** 100 RPM

*No specific free models listed currently.*

---

### [BentoML](https://www.bentoml.com/)

**Description:** An Inference Platform built for speed and control, enabling deployment of any AI/ML model anywhere with tailored optimization, efficient scaling, and streamlined operations. It offers a complete solution to simplify inference infrastructure while giving full control over deployments.  
**Limits:** Hardware dependent

*No specific free models listed currently.*

---

### [Cerebras](https://cerebras.ai/inference)

**Description:** Cerebras Systems offers the world's fastest AI inference service, powered by the Wafer-Scale Engine (WSE-3). It delivers instant speed for Llama and other open-source models, making it ideal for real-time applications and complex reasoning tasks.  
**Limits:** 30 RPM

<details>
<summary><strong>Available Models (2)</strong></summary>

- Llama 3.1 8B (Fast)
- Llama 3.1 70B (Fast)

</details>

---

### [Cerebrium](https://www.cerebrium.ai/)

**Description:** Serverless GPU infrastructure for AI models. Deploy any model in minutes with automatic scaling. New users receive $30 in free compute credits.  
**Limits:** Pay-per-second compute

*No specific free models listed currently.*

---

### [Chutes.ai](https://chutes.ai/)

**Description:** Free GPU-powered inference for open-source models. Chutes runs models on donated and idle GPU capacity, offering truly free access to models like Llama 3.1, DeepSeek, and more.  
**Limits:** Varies (community capacity)

<details>
<summary><strong>Available Models (3)</strong></summary>

- DeepSeek-R1
- Llama 3.1 70B Instruct
- Qwen 2.5 72B Instruct

</details>

---

### [Cloudflare Workers AI](https://dash.cloudflare.com/)

**Description:** Run AI models on Cloudflare's global network. Workers AI gives you a generous free tier of 10,000 neurons per day across dozens of open-source models including Llama, Mistral, and more. No credit card required.  
**Limits:** Varies by model

<details>
<summary><strong>Available Models (6)</strong></summary>

- Llama 3.1 8B Instruct
- Llama 3.2 3B Instruct
- Mistral 7B Instruct v0.2
- Qwen 1.5 7B Chat
- DeepSeek Coder 6.7B
- Phi-2

</details>

---

### [Cohere](https://cohere.com/)

**Description:** Models share a common monthly quota.  
**Limits:** 20 requests/minute

<details>
<summary><strong>Available Models (3)</strong></summary>

- Command R+ (08-2024)
- Command R (08-2024)
- Command R7B (12-2024)

</details>

---

### [Coze](https://www.coze.com/)

**Description:** ByteDance's AI platform offering free access to build and deploy AI chatbots and agents. Provides free API access with generous limits to multiple models including GPT-4o and Gemini.  
**Limits:** Varies by model

<details>
<summary><strong>Available Models (2)</strong></summary>

- GPT-4o (via Coze)
- Gemini 1.5 Pro (via Coze)

</details>

---

### [DeepInfra](https://deepinfra.com/)

**Description:** Cost-effective inference platform with $5 free credits on signup. Hosts 40+ open-source models with OpenAI-compatible API. Known for reliable uptime and competitive pricing after credits.  
**Limits:** 60 RPM (varies by model)

*No specific free models listed currently.*

---

### [DeepSeek](https://platform.deepseek.com/)

**Description:** Creators of DeepSeek-V3 and DeepSeek-R1, breakthrough open-source reasoning models. New users receive 10M free tokens. API is OpenAI-compatible with extremely competitive pricing after credits.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [Fireworks AI](https://fireworks.ai/)

**Description:** The fastest production platform for Generative AI. Run open-source models with blazing speed and efficiency. Specialized in fire-function calling and JSON mode.  
**Limits:** 600 RPM

*No specific free models listed currently.*

---

### [Friendli AI](https://friendli.ai/)

**Description:** Enterprise-grade serverless inference with $10 free trial credits. Optimized for latency and throughput with support for popular open-source models. OpenAI-compatible API.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [GitHub Models](https://github.com/marketplace/models)

**Description:** Free access to GPT-4o, Llama, Mistral and more through GitHub's model marketplace. Requires a GitHub account. Limits vary by Copilot tier.  
**Limits:** Varies by Copilot Tier

<details>
<summary><strong>Available Models (6)</strong></summary>

- GPT-4o
- Llama 3.3 70B Instruct
- Phi-4
- Mistral Large (24.11)
- Cohere Command R+
- AI21 Jamba 1.5 Large

</details>

---

### [Glhf.chat](https://glhf.chat/)

**Description:** Free serverless inference for open-source models. Access Llama, Mistral, and other models through an OpenAI-compatible API with generous free tier. Simple, developer-friendly platform.  
**Limits:** 30 RPM

<details>
<summary><strong>Available Models (3)</strong></summary>

- Llama 3.1 70B Instruct
- Mixtral 8x7B Instruct
- Phi-3 Mini 4K Instruct

</details>

---

### [Google AI Studio](https://aistudio.google.com/)

**Description:** Google AI Studio is a web-based prototyping environment for developers to experiment with Gemini models. It offers a generous free tier that includes access to the latest Gemini 1.5 and 2.0 models, including Flash and Pro versions. It is designed for fast iteration and development, providing a seamless path from prototype to production with the Gemini API.  
**Limits:** 2-15 RPM

<details>
<summary><strong>Available Models (4)</strong></summary>

- Gemini 2.0 Flash
- Gemini 2.0 Flash-Lite
- Gemini 1.5 Flash
- Gemini 1.5 Pro

</details>

---

### [GPT4All](https://gpt4all.io/)

**Description:** A free-to-use, locally running, privacy-aware chatbot. No GPU or internet required. Runs on popular consumer hardware using CPU quantization.  
**Limits:** Hardware dependent

<details>
<summary><strong>Available Models (3)</strong></summary>

- Snoozy
- Llama 3 8B Quant
- Nomic Embed

</details>

---

### [Grok (xAI)](https://console.x.ai/)

**Description:** xAI's Grok models with a generous free API tier: $25/month in free credits that renew monthly. Access Grok-2 and Grok-2 Mini through an OpenAI-compatible API. Strong reasoning and real-time knowledge.  
**Limits:** Varies (low for free tier)

<details>
<summary><strong>Available Models (3)</strong></summary>

- Grok-2
- Grok-2 Mini
- Grok-2 Vision

</details>

---

### [Groq](https://console.groq.com/)

**Description:** LPU Inference Engine.  
**Limits:** 30 RPM, 14.4k RPD

<details>
<summary><strong>Available Models (16)</strong></summary>

- Allam 2 7B
- Llama 3.1 8B
- Llama 3.3 70B
- Llama 4 Maverick 17B
- Llama 4 Scout
- Whisper Large v3
- Whisper Large v3 Turbo
- Groq Compound
- Groq Compound Mini
- Llama Guard 4 12B
- Moonshot Kimi K2
- Moonshot Kimi K2 0905
- GPT-OSS 120B
- GPT-OSS 20B
- GPT-OSS Safeguard 20B
- Qwen3 32B

</details>

---

### [Hugging Face Inference](https://huggingface.co/inference-api/serverless)

**Description:** The Hugging Face Serverless Inference API allows you to access over 100,000 publicly available machine learning models. It is designed for prototyping and testing, allowing you to run inference on models without managing infrastructure. While not for heavy production, it offers a generous free tier for experimentation.  
**Limits:** 300 Requests / hour

<details>
<summary><strong>Available Models (5)</strong></summary>

- Llama 3.2 11B Vision
- Llama 3.1 8B Instruct
- Qwen 2.5 72B Instruct
- Gemma 2 9B Instruct
- Flux.1 Dev

</details>

---

### [Hyperbolic](https://app.hyperbolic.xyz/)

**Description:** Decentralized AI inference network. Access top-tier open source models like Llama 3.1 405B and DeepSeek V3 at a fraction of the cost.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [Inference.net](https://inference.net/)

**Description:** Decentralized GPU network offering free inference for open-source models. Built on distributed compute, providing reliable access to Llama, DeepSeek, and other models at no cost.  
**Limits:** 30 RPM (fair use)

<details>
<summary><strong>Available Models (3)</strong></summary>

- DeepSeek-R1
- Llama 3.1 8B Instruct
- Llama 3.1 70B Instruct

</details>

---

### [Jan.ai](https://jan.ai/)

**Description:** Run open source AI locally on your desktop. Jan is a ChatGPT-alternative that runs 100% offline, privacy-focused, and provides an OpenAI-compatible local server.  
**Limits:** Hardware dependent

<details>
<summary><strong>Available Models (3)</strong></summary>

- Llama 3 (Local)
- Mistral (Local)
- Gemma 2 (Local)

</details>

---

### [Kluster.ai](https://kluster.ai/)

**Description:** Free batch inference API for LLMs. Optimized for high-throughput batch processing with support for Llama, Mistral, DeepSeek and more. Perfect for bulk text processing at zero cost.  
**Limits:** Batch-based (async)

<details>
<summary><strong>Available Models (3)</strong></summary>

- Llama 3.1 405B Instruct
- DeepSeek-R1
- Qwen 2.5 72B Instruct

</details>

---

### [KoboldCpp](https://github.com/LostRuins/koboldcpp)

**Description:** A single-file GGUF inference engine for LLMs. Oriented towards storytelling and roleplay, with rich features for context management and world info.  
**Limits:** Hardware dependent

<details>
<summary><strong>Available Models (1)</strong></summary>

- Any GGUF Model

</details>

---

### [Lepton AI](https://www.lepton.ai/)

**Description:** A developer-centric platform for building AI apps. Run simple, standard APIs for open source models like Llama, Mistral, and Stable Diffusion with auto-scaling.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [llama.cpp](https://github.com/ggerganov/llama.cpp)

**Description:** Port of Facebook's LLaMA model in C/C++. The foundational project that enables running LLMs on consumer hardware (Mac, Windows, Linux, Android) with high performance.  
**Limits:** Hardware dependent

<details>
<summary><strong>Available Models (1)</strong></summary>

- Any GGUF Model

</details>

---

### [llamafile](https://github.com/Mozilla-Ocho/llamafile)

**Description:** Distribute and run LLMs with a single file. Llamafile combines llama.cpp with Cosmopolitan Libc to create multi-platform executables that run anywhere.  
**Limits:** Hardware dependent

<details>
<summary><strong>Available Models (3)</strong></summary>

- LLaVA 1.5
- Mistral 7B
- TinyLlama

</details>

---

### [LM Studio](https://lmstudio.ai/)

**Description:** The easiest way to discover, download, and run local LLMs. Features a beautiful UI, GPU offloading, and a built-in local server that mimics OpenAI's API. Perfect for non-technical users.  
**Limits:** Hardware limited

<details>
<summary><strong>Available Models (4)</strong></summary>

- Llama 3.1 (Any Size)
- Gemma 2 (Any Size)
- Mistral (Any version)
- Phi-3 (Any version)

</details>

---

### [Mistral (La Plateforme)](https://console.mistral.ai/)

**Description:** Mistral Experiment plan. Requires opting into data training and phone verification.  
**Limits:** 1 request/second

<details>
<summary><strong>Available Models (4)</strong></summary>

- Mistral 7B
- Mixtral 8x7B
- Mistral Small
- Mistral Nemo

</details>

---

### [Nebius](https://studio.nebius.com/)

**Description:** Efficient AI inference studio. Access a wide range of open-source models with low latency and cost-effective pricing.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [Novita AI](https://novita.ai/)

**Description:** AI infrastructure for developers. Offers various open-source models including Llama and Mistral, with a focus on stability and ease of use.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [NVIDIA NIM](https://build.nvidia.com/explore/discover)

**Description:** NVIDIA Inference Microservices. Access various open-source models with free credits. Phone verification required.  
**Limits:** 40 requests/minute

*No specific free models listed currently.*

---

### [Ollama](https://ollama.com/)

**Description:** The standard for local AI. Run Llama 3, Mistral, Gemma, and hundreds of other models directly on your Mac, Linux, or Windows machine. Complete privacy, zero cost, and offline capability.  
**Limits:** Hardware limited

<details>
<summary><strong>Available Models (5)</strong></summary>

- Llama 3.2 3B
- Gemma 2 9B
- Mistral Nemo 12B
- Phi-3.5 Mini
- DeepSeek Coder V2

</details>

---

### [OpenRouter](https://openrouter.ai/)

**Description:** A community-focused router for LLMs. Offers a specific list of truly free models with shared quotas.  
**Limits:** 20 requests/minute

<details>
<summary><strong>Available Models (11)</strong></summary>

- Google: Gemini 2.0 Flash (free)
- Google: Gemini 2.0 Pro (free)
- Meta: Llama 3.3 70B Instruct (free)
- NVIDIA: Llama 3.1 Nemotron 70B (free)
- DeepSeek: R1 (free)
- DeepSeek: R1 Distill Llama 70B (free)
- Mistral: Small 3 (free)
- Qwen 2.5 7B Instruct (free)
- Qwen 2.5 VL 72B Instruct (free)
- Microsoft: Phi-3 Medium (free)
- Microsoft: Phi-3 Mini (free)

</details>

---

### [OVH AI Endpoints](https://endpoints.ai.cloud.ovh.net/)

**Description:** OVHcloud's AI Endpoints in Beta. Access open source models hosted in Europe including Qwen3Guard, Audio, and Image generation models.  
**Limits:** 2 RPM (Anonymous) / 400 RPM (Auth)

<details>
<summary><strong>Available Models (7)</strong></summary>

- Qwen3Guard-Gen-0.6B (Beta)
- Qwen3Guard-Gen-8B (Beta)
- stable-diffusion-xl-base-v10
- nvr-tts-es-es
- nvr-tts-de-de
- nvr-tts-en-us
- nvr-tts-it-it

</details>

---

### [Qwen (Alibaba)](https://bailian.console.alibabacloud.com/)

**Description:** The enterprise AI platform from Alibaba Cloud. Home of the Qwen (Tongyi Qianwen) model family, offering state-of-the-art performance in coding and mathematics.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [Replicate](https://replicate.com/)

**Description:** Run open-source models with a single line of code. Thousands of models available, from LLMs to Stable Diffusion, running on scalable GPU infrastructure.  
**Limits:** Varies by model

*No specific free models listed currently.*

---

### [Requesty](https://requesty.ai/)

**Description:** AI gateway and router with a built-in free tier. Route requests across multiple providers with automatic fallback, caching, and load balancing. Includes free credits monthly.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [SambaNova Cloud](https://cloud.sambanova.ai/)

**Description:** SambaNova Cloud delivers the world's fastest inference for open-source models like Llama 3.1 405B and Qwen 2.5, powered by the purpose-built SN40L Reconfigurable Dataflow Unit (RDU). It offers lightning-fast speed and a generous free credit for new users.  
**Limits:** Varies by model

*No specific free models listed currently.*

---

### [Scaleway Generative APIs](https://console.scaleway.com/generative-api/models)

**Description:** European cloud provider offering managed generative AI APIs. Host to Mistral, Llama, and Qwen models with full GDPR compliance and data sovereignty.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [Text Generation WebUI](https://github.com/oobabooga/text-generation-webui)

**Description:** The Swiss Army Knife of local LLMs. Highly customizable Gradio interface for running Large Language Models like Llama, GPT-J, OPT, and GALACTICA locally.  
**Limits:** Hardware dependent

<details>
<summary><strong>Available Models (1)</strong></summary>

- Any Local Model

</details>

---

### [Together.AI](https://together.ai/)

**Description:** Access specific free research models from ServiceNow and others. Currently hosting the Apriel Thinker series for free.  
**Limits:** Subject to availability

<details>
<summary><strong>Available Models (2)</strong></summary>

- Apriel 1.6 15B Thinker (Free)
- Apriel 1.5 15B Thinker (Free)

</details>

---

### [Upstage](https://console.upstage.ai/)

**Description:** Leading AI company specializing in DUS (Document Understanding) and Solar LLMs. Solar Pro delivers GPT-4 level performance with remarkable speed and efficiency.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

### [Venice.ai](https://venice.ai/)

**Description:** Privacy-first AI inference. Venice guarantees 100% privacy with no data logging, running open weights models on decentralized GPU nodes.  
**Limits:** 10 RPM (free tier)

<details>
<summary><strong>Available Models (3)</strong></summary>

- Llama 3.1 405B
- Dolphin Mixtral
- Stable Diffusion 3

</details>

---

### [Yi AI](https://www.01.ai/)

**Description:** 01.AI's flagship open-source models. Yi-Large provides GPT-4 class performance with strong reasoning capabilities and a 200k context window.  
**Limits:** 60 RPM

*No specific free models listed currently.*

---

## ü§ù Contributing

This list is maintained automatically. If you know of a free provider that isn't listed, please verify it on [free-llm-api-resources](https://github.com/cheahjs/free-llm-api-resources) as we sync from there.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
*Generated by [Free-LLM.com](https://free-llm.com)*
